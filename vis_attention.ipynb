{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.6.9 (default, Jan 26 2021, 15:33:00) \n",
      "[GCC 8.4.0]\n",
      "torch: 1.7.1\n",
      "use_cuda: False, n_gpu: 0, device: cpu, devices:[]\n",
      "transformers: 4.2.0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys, os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "sys.path.append(os.path.expanduser(\"~\")+\"/dotfiles\")\n",
    "import myutils\n",
    "\n",
    "import time, random, pickle, logging, uuid, collections, copy, json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy, sklearn\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "use_cuda, device, n_gpu = myutils.print_torch(torch)\n",
    "\n",
    "import transformers\n",
    "myutils.print_packages(transformers)\n",
    "from transformers import *\n",
    "from datasets import *\n",
    "\n",
    "SEED = 42\n",
    "myutils.seed_everything(SEED, random, os, np, torch)\n",
    "cache_dir = '~/.cache'\n",
    "\n",
    "from mammo2text import BreastImgTextDataset, MammoImgTextDataCollator, get_target_text, get_mammo2text_model, \\\n",
    "    generate_predictions, AddParametersToMlflowCallback, generate, load_image_model, EncoderWrapper\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import math \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 40003\n"
     ]
    }
   ],
   "source": [
    "model_name = '1_att_cnn_niar4'\n",
    "\n",
    "parameters = {}\n",
    "parameters = myutils.json_load(f'models/{model_name}/parameters.json')\n",
    "parameters['debug'] = False\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(parameters[\"tokenizer_path\"])\n",
    "print('Vocab size:', len(tokenizer.get_vocab()))\n",
    "\n",
    "random_number_generator = np.random.RandomState()\n",
    "\n",
    "exam_list_eval = myutils.json_load(parameters[\"eval_json_path\"]) #[:10]\n",
    "\n",
    "##### Select best rated logic\n",
    "res = []\n",
    "for x in best_rated_cases[:10]:\n",
    "    for y in exam_list_eval:\n",
    "        if y['study_uid'] == x:\n",
    "            res.append(y)\n",
    "exam_list_eval = res           \n",
    "##############################\n",
    "\n",
    "\n",
    "eval_dataset = BreastImgTextDataset(\n",
    "    exam_list=exam_list_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    parameters=parameters, \n",
    "    random_number_generator=random_number_generator, \n",
    "    swap=False\n",
    ")\n",
    "\n",
    "data_collator = MammoImgTextDataCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = myutils.excel.read('data/label2/all_exp.xlsx')\n",
    "best_model = 'pred_1_cnn_decoder_att'\n",
    "model3 = 'pred_1_random_cnn_random_decoder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = label_df[label_df['type'].isin(['TARGET', best_model, model3])][['type', 'Предсказание', 'Общая оценка от 1-10', 'case_id', 'Номер случая', 'Номер строки']]\n",
    "\n",
    "b = a[a['type']==best_model].sort_values('Общая оценка от 1-10', ascending=False)\n",
    "b.set_index('case_id', inplace=True)\n",
    "\n",
    "b2 = a[a['type']=='TARGET']\n",
    "b2.set_index('case_id', inplace=True)\n",
    "b2 = b2.rename(columns={x: 'target-'+x for x in b2.columns})\n",
    "\n",
    "b3 = a[a['type']==model3]\n",
    "b3.set_index('case_id', inplace=True)\n",
    "b3 = b3.rename(columns={x: 'model-bad-'+x for x in b3.columns})\n",
    "\n",
    "c = pd.merge(b, b3, left_index=True, right_index=True)\n",
    "c = pd.merge(c, b2, left_index=True, right_index=True)\n",
    "c['picture_id'] = range(len(c))\n",
    "\n",
    "myutils.excel.save(c, 'data/label_viz/predictions2.xlsx', long_columns=['Предсказание', 'model-bad-Предсказание', 'target-Предсказание'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rated_cases = c.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myutils.display_df(b[['Предсказание']].iloc[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mammo2text_model(parameters, tokenizer, device, load_image_model, EncoderWrapper)\n",
    "model.load_state_dict(torch.load(f'models/{parameters[\"model_name\"]}/pytorch_model.bin'))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_hooks(model):\n",
    "    hook_nodes = {\n",
    "        'decoder.bert.encoder.layer.10.crossattention.self.dropout', \n",
    "        'decoder.bert.encoder.layer.11.crossattention.self.dropout',\n",
    "#         'encoder.model.four_view_net.net._conv_head',\n",
    "#          'encoder.model.four_view_net.net._conv_head.static_padding',\n",
    "#          'encoder.model.four_view_net.net._bn1'\n",
    "    }\n",
    "\n",
    "    activations = collections.defaultdict(list)\n",
    "    def save_activation(name, mod, inp, out):\n",
    "        activations[name] = out.cpu()\n",
    "\n",
    "    for name, m in model.named_modules():\n",
    "        if name in hook_nodes:\n",
    "            handle = m.register_forward_hook(partial(save_activation, name))\n",
    "    return activations\n",
    "    \n",
    "model_with_hooks = copy.deepcopy(model)\n",
    "activations = register_hooks(model_with_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "order:\n",
    "\n",
    "`hh = torch.stack([h[VIEWS.L_CC], h[VIEWS.R_CC], h[VIEWS.L_MLO], h[VIEWS.R_MLO]], -1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(a, bins=50):\n",
    "    if isinstance(a, torch.Tensor):\n",
    "        a = a.cpu().detach().numpy()\n",
    "    _ = plt.hist(a, bins=bins)\n",
    "    plt.show()\n",
    "    \n",
    "def get_named_modules(model):\n",
    "    return [x for x,y in model.named_modules()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_array(t, save=False):\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        t = t.cpu().detach().numpy()\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(t , cmap='gray')\n",
    "    if save:\n",
    "        _ = plt.savefig('viz/picture.png')\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id=0\n",
    "sample = eval_dataset[sample_id]\n",
    "image = os.path.join('', exam_list_eval[sample_id]['L-CC'][0])\n",
    "image_array = np.array(Image.open(image))\n",
    "viz_array(image_array)\n",
    "viz_array(eval_dataset[sample_id]['L-CC'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_in_subplots(img_array, file_name='viz/subplots.png'):\n",
    "    plt.ioff()\n",
    "    fig = plt.figure()\n",
    "    square = math.ceil(len(img_array) ** (1/2))\n",
    "    ix = 1\n",
    "    while ix-1 < len(img_array):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = plt.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # img_array = feature_maps[0, ix-1, :, :]\n",
    "        _ = plt.imshow(img_array[ix-1], cmap='gray', vmin=0, vmax=12)\n",
    "        ix += 1\n",
    "    _ = plt.savefig(file_name, dpi=1000)\n",
    "    plt.close(fig)\n",
    "    print(f'Saved as {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_att = np.array([\n",
    "#                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#                 [0, 0, 0, 0, 0, 0, 0.9, 0.9, 0, 0],\n",
    "#                 [0, 0, 0, 0, 0, 0.9, 1, 1, 0, 0],\n",
    "#                 [0, 0, 0, 0, 0, 0.9, 1, 1, 0, 0],\n",
    "#                 [0, 0, 0, 0, 0, 0, 0.9, 0.9, 0, 0],\n",
    "#                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#                 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "#             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl = None\n",
    "def plot_attention(images, words, attention_plot, file_name='viz/attention.png', \n",
    "                   cmap='jet', interpolation='spline16', alpha=None, quality_factor=1, \n",
    "                   use_add_alpha=True, image_id=0):\n",
    "    global gl\n",
    "    plt.ioff()\n",
    "    len_words = len(words) \n",
    "    len_images = len(images)\n",
    "    layout_words = len_words+1\n",
    "    layout_images = len_images\n",
    "    fig = plt.figure(figsize=(layout_words*1.5*quality_factor, layout_images*3*quality_factor))\n",
    "    pbar = tqdm(total=len_words*len_images)\n",
    "    for img_index in range(len_images):\n",
    "        temp_image = images[img_index]\n",
    "        i = layout_words*img_index + 1\n",
    "        ax = fig.add_subplot(layout_images, layout_words, i)\n",
    "        img = ax.imshow(temp_image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    attention_plot = attention_plot.sum(0)\n",
    "    attention_plot = np.expand_dims(attention_plot, 0)\n",
    "    print(attention_plot.shape)\n",
    "    for word_index in range(len_words):\n",
    "        word_min = attention_plot[word_index,:,:,:].min()\n",
    "        word_max = attention_plot[word_index,:,:,:].max()\n",
    "        for img_index in range(len_images):\n",
    "            temp_image = images[img_index]\n",
    "            temp_att = attention_plot[word_index,:,:, img_index]\n",
    "            i = layout_words*img_index + word_index + 2\n",
    "            ax = fig.add_subplot(layout_images, layout_words, i)\n",
    "            plt.axis('off')\n",
    "            if img_index == 0:\n",
    "                ax.set_title(result[word_index])\n",
    "            img = ax.imshow(temp_image, cmap='gray')\n",
    "            \n",
    "            p = np.percentile(temp_att, 50)\n",
    "            temp_att[temp_att < p] = p\n",
    "            \n",
    "            _alpha = alpha\n",
    "            if use_add_alpha:\n",
    "                _alpha = np.ones(temp_att.shape) * alpha\n",
    "                scaler = MinMaxScaler((-0.2, 0.1))\n",
    "                additional_alpha = scaler.fit_transform(temp_att)\n",
    "                _alpha += additional_alpha\n",
    "\n",
    "            ax.imshow(temp_att, interpolation=interpolation, cmap=cmap, alpha=_alpha, \n",
    "                      extent=img.get_extent(), vmin=word_min, vmax=word_max) #, vmin=word_min, vmax=word_max , vmin=-0.1, vmax=1.1 , vmin=0, vmax=1 # , vmin=temp_att.min(), vmax=temp_att.max()\n",
    "            pbar.update(1)\n",
    "    fig.tight_layout()\n",
    "    _ = plt.savefig(f'viz/{image_id}_att.png', dpi=100)\n",
    "    pbar.close()\n",
    "    plt.close(fig)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.6",
   "language": "python",
   "name": "venv3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
